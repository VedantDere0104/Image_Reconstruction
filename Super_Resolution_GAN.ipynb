{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Super_Resolution_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1NP844Opxd3XkTx99Ob2e8YrG04rcu3dK",
      "authorship_tag": "ABX9TyOblJr2EMmubM9RqV//qLt/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VedantDere0104/Image_Reconstruction/blob/main/Super_Resolution_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fr2la0E67f3"
      },
      "source": [
        "####"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piEXrRB07Ak0"
      },
      "source": [
        "#! unzip '/content/drive/MyDrive/Wider_Face_Dataset/WIDER_train.zip' -d '/content/drive/MyDrive/Wider_Face_Dataset/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljtJMWrFHLZ1"
      },
      "source": [
        "import torchvision\r\n",
        "from torchvision import datasets\r\n",
        "from torchvision import transforms\r\n",
        "from torch.utils.data import DataLoader , Dataset\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tqdm.auto import tqdm\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torchsummary import summary\r\n",
        "import cv2 as cv\r\n",
        "import numpy as np\r\n",
        "from torchvision.utils import make_grid"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf6a3oT5dbuS"
      },
      "source": [
        "def show_tensor_images(image_tensor, num_images=2, size=(1, 28, 28) , switch = True):\r\n",
        "  image_shifted = image_tensor\r\n",
        "  #print(image_shifted)\r\n",
        "  image_unflat = image_shifted.detach().cpu().view(-1, *size)\r\n",
        "  #print(image_unflat)\r\n",
        "  image_grid = make_grid(image_unflat[:num_images], nrow=2 , normalize=False)\r\n",
        "  #print(image_grid)\r\n",
        "  if switch:\r\n",
        "    image_grid = image_grid * 255.0\r\n",
        "  plt.imshow(image_grid.permute(1 , 2, 0).squeeze())\r\n",
        "  plt.show()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Biwf0ncO5JD"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fASVzl9Q7Udj"
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor() , \r\n",
        "                                transforms.ToPILImage()  , \r\n",
        "                                transforms.Resize((512 , 512) ) , \r\n",
        "                                transforms.ToTensor()])\r\n",
        "\r\n",
        "low_res_dataset = torchvision.datasets.ImageFolder(\"/content/drive/MyDrive/Wider_Face_Dataset/Cleaned_Dataset/\", transform=transform)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg8ueXJZbOtC"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72-TeI-2UBqw"
      },
      "source": [
        "high_res_dataset = torchvision.datasets.ImageFolder(\"/content/drive/MyDrive/Wider_Face_Dataset/High_res_dataset\" , transform=transform)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w-1MTiNUW4G"
      },
      "source": [
        "class Low_High_Dataset(Dataset):\r\n",
        "  def __init__(self , \r\n",
        "               low_res_dataset , \r\n",
        "               high_res_dataset):\r\n",
        "    super(Low_High_Dataset , self).__init__()\r\n",
        "\r\n",
        "    self.low_res_dataset = low_res_dataset\r\n",
        "    self.high_res_dataset = high_res_dataset\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.low_res_dataset)\r\n",
        "\r\n",
        "  def __getitem__(self , idx):\r\n",
        "    x = self.low_res_dataset\r\n",
        "    y = self.high_res_dataset\r\n",
        "    x = x[idx][0]\r\n",
        "    y = y[idx][0]\r\n",
        "    #print('x ->' , x)\r\n",
        "    #print('y ->' , y)\r\n",
        "    return (x , y)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQvVHGocWTCa"
      },
      "source": [
        "dataset = Low_High_Dataset(low_res_dataset , high_res_dataset)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZOaI1WvWny7"
      },
      "source": [
        "batch_size = 2\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSVIJaJZWXon"
      },
      "source": [
        "dataloader = DataLoader(dataset , batch_size , True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnjbLxMuWZK-"
      },
      "source": [
        "dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIXJzo8HXHYI"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrrQx2d6cN4h"
      },
      "source": [
        "\r\n",
        "def crop(image, new_shape):\r\n",
        "  middle_height = image.shape[2] // 2\r\n",
        "  middle_width = image.shape[3] // 2\r\n",
        "  starting_height = middle_height - new_shape[2] // 2\r\n",
        "  final_height = starting_height + new_shape[2]\r\n",
        "  starting_width = middle_width - new_shape[3] // 2\r\n",
        "  final_width = starting_width + new_shape[3]\r\n",
        "  cropped_image = image[:, :, starting_height:final_height, starting_width:final_width]\r\n",
        "  return cropped_image"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3pn15QkdFca"
      },
      "source": [
        "\r\n",
        "class Helper_1(nn.Module):\r\n",
        "  def __init__(self , in_channels , out_channels , kernel_size = (2 , 2) , stride = (2 , 2) , use_batch_norm = True , activation = 'lreu'):\r\n",
        "    super(Helper_1 , self).__init__()\r\n",
        "\r\n",
        "    self.use_batch_norm = use_batch_norm\r\n",
        "    self.conv1 = nn.Conv2d(in_channels , out_channels , kernel_size , stride)\r\n",
        "    self.batch_norm = nn.InstanceNorm2d(out_channels)\r\n",
        "    self.activation = activation\r\n",
        "    if self.activation == 'relu':\r\n",
        "      self.relu = nn.ReLU()\r\n",
        "    else :\r\n",
        "      self.relu = nn.LeakyReLU(0.2)\r\n",
        "\r\n",
        "  def forward(self , x):\r\n",
        "    #print(x.shape)\r\n",
        "    x = self.conv1(x)\r\n",
        "    x = self.batch_norm(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    #print(x.shape)\r\n",
        "    return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCZDpOCLdIn5"
      },
      "source": [
        "\r\n",
        "class Encoder(nn.Module):\r\n",
        "  def __init__(self , in_channels , hidden_dim , out_channels):\r\n",
        "    super(Encoder , self).__init__()\r\n",
        "\r\n",
        "    self.conv1 = Helper_1(in_channels , hidden_dim , (2 , 2) , (2 , 2) , False , 'relu')\r\n",
        "    self.conv2 = Helper_1(hidden_dim , hidden_dim * 2 , (2 , 2) , (2 , 2) , True , 'relu')\r\n",
        "    self.conv3 = Helper_1(hidden_dim * 2 , hidden_dim * 4 , (2 , 2) , (2 , 2) , True , 'relu')\r\n",
        "    self.conv4 = Helper_1(hidden_dim * 4 , hidden_dim * 8 , (2 , 2) , (2 , 2) ,True, 'relu')\r\n",
        "    self.conv5 = Helper_1(hidden_dim * 8 , hidden_dim * 16 , (2 , 2) , (2 ,2) , True, 'relu')\r\n",
        "    self.conv6 = Helper_1(hidden_dim * 16 , hidden_dim * 32 , (2 , 2) ,(2 , 2) , True, 'relu')\r\n",
        "    self.conv7 = Helper_1(hidden_dim * 32 , hidden_dim * 64 , (2 , 2) , (2 , 2) , True, 'relu')\r\n",
        "    self.conv8 = Helper_1(hidden_dim * 64 , hidden_dim * 32 , (2 , 2) , (2 , 2) , True, 'relu')\r\n",
        "    self.flatten = nn.Flatten()\r\n",
        "    self.linear1 = nn.Linear(4096 , hidden_dim * 16)\r\n",
        "    self.batchnorm = nn.BatchNorm1d(hidden_dim * 16)\r\n",
        "    self.relu = nn.ReLU()\r\n",
        "    \r\n",
        "    self.linear2 = nn.Linear(hidden_dim * 16 , hidden_dim * 16)\r\n",
        "\r\n",
        "\r\n",
        "  def forward(self , x):\r\n",
        "    x = self.conv1(x)\r\n",
        "    x = self.conv2(x) \r\n",
        "    x = self.conv3(x)\r\n",
        "    x = self.conv4(x) \r\n",
        "    x = self.conv5(x)\r\n",
        "    x = self.conv6(x)\r\n",
        "    x = self.conv7(x)\r\n",
        "    x = self.conv8(x)\r\n",
        "    x = self.flatten(x)\r\n",
        "    x = self.relu(self.linear1(x))\r\n",
        "    x = self.linear2(x)\r\n",
        "    x = x.view(x.shape[0] , x.shape[1] , 1 , 1)\r\n",
        "    return x"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaomRP8edLJ5"
      },
      "source": [
        "\r\n",
        "class Helper_2(nn.Module):\r\n",
        "  def __init__(self , in_channels , out_channels , kernel_size = (2 , 2) , stride = (2 , 2) , use_batchnorm = True , activation = 'relu'):\r\n",
        "\r\n",
        "    super(Helper_2 , self).__init__()\r\n",
        "\r\n",
        "    self.use_batchnorm = use_batchnorm\r\n",
        "    self.convT1 = nn.ConvTranspose2d(in_channels , out_channels , \r\n",
        "                                     kernel_size , stride)\r\n",
        "    \r\n",
        "    if self.use_batchnorm:\r\n",
        "      self.batchnorm = nn.InstanceNorm2d(out_channels)\r\n",
        "\r\n",
        "    self.activation = activation\r\n",
        "    if self.activation == 'lrelu':\r\n",
        "      self.lrelu = nn.LeakyReLU()\r\n",
        "    elif self.activation == 'relu':\r\n",
        "      self.relu = nn.ReLU()\r\n",
        "    \r\n",
        "\r\n",
        "  def forward(self , x):\r\n",
        "    x = self.convT1(x)\r\n",
        "    if self.use_batchnorm:\r\n",
        "      x = self.batchnorm(x)\r\n",
        "    if self.activation == 'lrelu':\r\n",
        "      x = self.lrelu(x)\r\n",
        "    elif self.activation == 'relu':\r\n",
        "      x = self.relu(x)\r\n",
        "    return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBZnoWuUdNSC"
      },
      "source": [
        "\r\n",
        "class Generator(nn.Module):\r\n",
        "  def __init__(self , z_in_channels , img_in_channels , hidden_dim , out_channels):\r\n",
        "    super(Generator , self).__init__()\r\n",
        "\r\n",
        "    self.encoder = Encoder(3 , 32 , 512)\r\n",
        "\r\n",
        "    self.convT1 = Helper_2(z_in_channels , hidden_dim , use_batchnorm=False)\r\n",
        "    self.convT2 = Helper_2(hidden_dim , hidden_dim  *2 , use_batchnorm=True)\r\n",
        "    self.convT3 = Helper_2(hidden_dim * 2 , hidden_dim * 4 , use_batchnorm=True)\r\n",
        "    self.convT4 = Helper_2(hidden_dim * 4 , hidden_dim * 8 , use_batchnorm=True)\r\n",
        "    self.convT5 = Helper_2(hidden_dim * 8 , hidden_dim * 16 , use_batchnorm=True)\r\n",
        "    self.convT6 = Helper_2(hidden_dim * 16 , hidden_dim  * 32 , use_batchnorm=True)\r\n",
        "    self.convT7 = Helper_2(hidden_dim * 32 , hidden_dim * 32 , use_batchnorm=False)\r\n",
        "\r\n",
        "    self.conv1 = Helper_1(img_in_channels , hidden_dim , use_batch_norm=False)\r\n",
        "    self.conv2 = Helper_1(hidden_dim , hidden_dim * 32 , use_batch_norm=False)\r\n",
        "\r\n",
        "    self.convT_1 = Helper_2(hidden_dim * 32 * 2 , hidden_dim * 32 , use_batchnorm=False)\r\n",
        "    self.convT_2 = Helper_2(hidden_dim * 32 , 3 , use_batchnorm=False , activation='relu')\r\n",
        "\r\n",
        "\r\n",
        "  def forward(self , x , y):\r\n",
        "    x = self.encoder(x)\r\n",
        "    x = self.convT1(x)\r\n",
        "    x = self.convT2(x)\r\n",
        "    x = self.convT3(x)\r\n",
        "    x = self.convT4(x)\r\n",
        "    x = self.convT5(x)\r\n",
        "    x = self.convT6(x)\r\n",
        "    x = self.convT7(x)\r\n",
        "\r\n",
        "    y = self.conv1(y)\r\n",
        "    y = self.conv2(y)\r\n",
        "\r\n",
        "    z = torch.cat([x , y] , dim=1)\r\n",
        "\r\n",
        "    z = self.convT_1(z)\r\n",
        "    z = self.convT_2(z)\r\n",
        "\r\n",
        "    return z\r\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfLbWHVJdPxK"
      },
      "source": [
        "class Discriminator(nn.Module):\r\n",
        "  def __init__(self , in_channels , hidden_dim , out_channels):\r\n",
        "    super(Discriminator , self).__init__()\r\n",
        "\r\n",
        "    self.conv1 = Helper_1(in_channels , hidden_dim , use_batch_norm=False)\r\n",
        "    self.conv2 = Helper_1(hidden_dim , hidden_dim * 2 )\r\n",
        "    self.conv3 = Helper_1(hidden_dim * 2 , hidden_dim * 4)\r\n",
        "    self.conv4 = Helper_1(hidden_dim * 4 , hidden_dim * 8)\r\n",
        "    self.conv5 = Helper_1(hidden_dim * 8 , hidden_dim * 16)\r\n",
        "    self.conv6 = Helper_1(hidden_dim * 16 , hidden_dim * 32)\r\n",
        "    self.conv7 = Helper_1(hidden_dim * 32 , hidden_dim * 32)\r\n",
        "    self.conv8 = Helper_1(hidden_dim * 32 , hidden_dim * 32)\r\n",
        "    self.flatten = nn.Flatten()\r\n",
        "\r\n",
        "    self.relu = nn.ReLU()\r\n",
        "    self.linear1 = nn.Linear(4096 , hidden_dim * 32)\r\n",
        "    self.batchnorm1 = nn.BatchNorm1d(hidden_dim * 32)\r\n",
        "    self.linear2 = nn.Linear(hidden_dim * 32 , hidden_dim * 8)\r\n",
        "    self.batchnorm2 = nn.BatchNorm1d(hidden_dim * 8)\r\n",
        "    self.linear3 = nn.Linear(hidden_dim * 8 , hidden_dim)\r\n",
        "    self.batchnorm3 = nn.BatchNorm1d(hidden_dim)\r\n",
        "    self.linear4 = nn.Linear(hidden_dim , out_channels)\r\n",
        "    self.sigmoid = nn.Sigmoid()\r\n",
        "\r\n",
        "\r\n",
        "  def forward(self , x):\r\n",
        "    x = self.conv1(x)\r\n",
        "    x = self.conv2(x)\r\n",
        "    x = self.conv3(x)\r\n",
        "    x = self.conv4(x)\r\n",
        "    x = self.conv5(x)\r\n",
        "    x = self.conv6(x)\r\n",
        "    x = self.conv7(x)\r\n",
        "    x = self.conv8(x)\r\n",
        "    x = self.flatten(x)\r\n",
        "    x = self.relu(self.linear1(x))\r\n",
        "    x = self.relu(self.linear2(x))\r\n",
        "    x = self.relu(self.linear3(x))\r\n",
        "    x = self.sigmoid(self.linear4(x))\r\n",
        "    return x"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUkIjOnJdSA5"
      },
      "source": [
        "generator_x = Generator(512 , 3 , 32 , 3).to(device)\r\n",
        "generator_y = Generator(512 , 3 , 32 , 3).to(device)\r\n",
        "generator_ = Generator(512 , 3 , 32 , 3).to(device)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suUz6RIYdWG_"
      },
      "source": [
        "discriminator_x = Discriminator(3 , 32 , 1).to(device)\r\n",
        "discriminator_y = Discriminator(3 , 32 , 1).to(device)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u36F6isKdZGn"
      },
      "source": [
        "\r\n",
        "def weights_init(m):\r\n",
        "  if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\r\n",
        "    torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n",
        "  if isinstance(m, nn.BatchNorm2d):\r\n",
        "    torch.nn.init.normal(m.weight, 0.0, 0.02)\r\n",
        "    torch.nn.init.constant(m.bias, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBwEJCyndarP"
      },
      "source": [
        "\r\n",
        "generator_x = generator_x.apply(weights_init)\r\n",
        "generator_y = generator_y.apply(weights_init)\r\n",
        "discriminator_x = discriminator_x.apply(weights_init)\r\n",
        "discriminator_y = discriminator_y.apply(weights_init)\r\n",
        "generator_ = generator_.apply(weights_init)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDbXDhtudcEv"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\r\n",
        "l1_loss = nn.L1Loss()\r\n",
        "lambda_recon = 200"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssBgvLSnddsv"
      },
      "source": [
        "\r\n",
        "\r\n",
        "betas = (0.5 , 0.999)\r\n",
        "n_epochs = 100\r\n",
        "input_dim = 3\r\n",
        "real_dim = 3\r\n",
        "display_step = 10\r\n",
        "lr = 0.0002\r\n",
        "target_shape = 512"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K27FryEudfTX"
      },
      "source": [
        "cur_step = 0\r\n",
        "mean_generator_loss = 0\r\n",
        "mean_discriminator_loss = 0"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evS_K-jUdlFZ"
      },
      "source": [
        "opt_generator_y = torch.optim.Adam(generator_y.parameters() , lr=lr , betas=betas)\r\n",
        "opt_generator_x = torch.optim.Adam(generator_x.parameters() , lr = lr , betas = betas)\r\n",
        "opt_discriminator_x = torch.optim.Adam(discriminator_x.parameters() , lr = lr , betas=betas)\r\n",
        "opt_discriminator_y = torch.optim.Adam(discriminator_y.parameters() , lr = lr , betas=betas)\r\n",
        "opt_generator_ = torch.optim.Adam(generator_.parameters() , lr=lr , betas=betas)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMlAPsQkdpMZ"
      },
      "source": [
        "def get_loss(fake , real  , criterion = criterion , l1_loss = l1_loss , lambda_recon = lambda_recon):\r\n",
        "  gen_loss = criterion(fake , real)\r\n",
        "  l1_loss = l1_loss(fake , real)\r\n",
        "  loss = gen_loss + lambda_recon *  l1_loss\r\n",
        "  return loss"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUrfwewRoK2S"
      },
      "source": [
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn0WwImAnDJP"
      },
      "source": [
        "threshold = 80"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4ePnR3JEn7c"
      },
      "source": [
        "for epoch in range(n_epochs):\r\n",
        "  for low_res , high_res in tqdm(dataloader):\r\n",
        "    real = low_res.to(device)\r\n",
        "    condition = high_res.to(device)\r\n",
        "    cur_batch_size = len(condition)  \r\n",
        "\r\n",
        "    if cur_step < threshold:\r\n",
        "      #print('Generator x is in grad mode')\r\n",
        "      opt_generator_y.zero_grad()\r\n",
        "      # Generator y -> high to low\r\n",
        "      hl1 = generator_y(condition , real)\r\n",
        "      hl2 = generator_y(condition , condition)\r\n",
        "\r\n",
        "      #loss_Y_XY = criterion(Y_XY , condition)\r\n",
        "      loss_Y_XY = get_loss(hl1 , condition)\r\n",
        "      #loss_Y_YY = criterion(Y_YY , condition)\r\n",
        "      loss_Y_YY = get_loss(hl2 , real)\r\n",
        "      loss_Y = (loss_Y_XY + loss_Y_YY) /2\r\n",
        "\r\n",
        "      loss_Y.backward()\r\n",
        "      opt_generator_y.step()\r\n",
        "    elif cur_step > threshold:\r\n",
        "      #print('Generator x is in no grad mode')\r\n",
        "      with torch.no_grad():\r\n",
        "        hl1 = generator_x(condition , real)\r\n",
        "        hl2 = generator_x(condition , condition)\r\n",
        "        loss_Y = 0\r\n",
        "\r\n",
        "    opt_generator_.zero_grad()\r\n",
        "    with torch.no_grad():\r\n",
        "      hl2 = generator_y(condition , condition)\r\n",
        "    lh = generator_(hl2 , hl2)\r\n",
        "    loss_ = get_loss(lh , condition)\r\n",
        "    loss_.backward()\r\n",
        "    opt_generator_.step()\r\n",
        "\r\n",
        "    opt_generator_x.zero_grad()\r\n",
        "    # Generator x -> low to high\r\n",
        "    with torch.no_grad():\r\n",
        "      hl1 = generator_y(condition , real)\r\n",
        "      hl2 = generator_y(condition , condition)\r\n",
        "      lh = generator_(hl2 , hl2)\r\n",
        "      \r\n",
        "    lh1 = generator_x(hl1 , lh)\r\n",
        "    lh2 = generator_x(lh , lh)\r\n",
        "\r\n",
        "    #loss_X_XX = criterion(X_XX , real)\r\n",
        "    loss_X_XX = get_loss(lh1 , condition)\r\n",
        "    #loss_X_YX = criterion(X_YX , real)\r\n",
        "    loss_X_YX = get_loss(lh2 , condition)\r\n",
        "    loss_X = (loss_X_XX + loss_X_YX) /2\r\n",
        "    loss_X.backward()\r\n",
        "    opt_generator_x.step()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    opt_discriminator_y.zero_grad()\r\n",
        "    with torch.no_grad():\r\n",
        "      disc_hl1 = generator_y(condition , real)\r\n",
        "      disc_hl2 = generator_y(condition ,condition)\r\n",
        "      disc_lh1 = generator_x(disc_hl1 , condition)\r\n",
        "      disc_lh2 = generator_x(disc_hl1 , condition)\r\n",
        "      disc_lh = generator_(disc_hl2 , disc_hl2)\r\n",
        "    \r\n",
        "    disc_fake_lh = discriminator_y(disc_lh)\r\n",
        "    disc_ = criterion(disc_fake_lh , torch.zeros_like(disc_fake_lh))\r\n",
        "\r\n",
        "    disc_fake_hl1 = discriminator_y(disc_hl1)\r\n",
        "    disc_loss_fake_pred_hl1 = criterion(disc_fake_hl1 , torch.zeros_like(disc_fake_hl1))\r\n",
        "\r\n",
        "    disc_fake_hl2 = discriminator_y(disc_hl2)\r\n",
        "    disc_loss_fake_pred_hl2 = criterion(disc_fake_hl2 , torch.zeros_like(disc_fake_hl2))\r\n",
        "\r\n",
        "    disc_fake_lh1 = discriminator_y(disc_lh1)\r\n",
        "    disc_loss_fake_pred_lh1 = criterion(disc_fake_lh1 , torch.zeros_like(disc_fake_lh1))\r\n",
        "\r\n",
        "    disc_fake_lh2 = discriminator_y(disc_lh2)\r\n",
        "    disc_loss_fake_pred_lh2 = criterion(disc_fake_lh2 , torch.zeros_like(disc_fake_lh2))\r\n",
        "\r\n",
        "\r\n",
        "    disc_real_pred = discriminator_y(condition)\r\n",
        "    disc_real_pred_loss = criterion(disc_real_pred , torch.ones_like(disc_real_pred))\r\n",
        "\r\n",
        "    disc_y_loss = (disc_loss_fake_pred_hl1 + disc_loss_fake_pred_hl2 + disc_real_pred_loss + disc_loss_fake_pred_lh1 + disc_loss_fake_pred_lh2 + disc_) /6\r\n",
        "\r\n",
        "    #print(disc_y_loss)\r\n",
        "\r\n",
        "    disc_y_loss.backward()\r\n",
        "    opt_discriminator_y.step()\r\n",
        "\r\n",
        "    disc_loss =  disc_y_loss\r\n",
        "    gen_loss = (loss_X + loss_Y)/2\r\n",
        "\r\n",
        "    mean_discriminator_loss += disc_loss.item() / display_step\r\n",
        "    mean_generator_loss += gen_loss.item() / display_step\r\n",
        "\r\n",
        "    if cur_step % display_step == 0:\r\n",
        "      if cur_step > 0:\r\n",
        "        print(f\"Epoch {epoch}: Step {cur_step}: Generator loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\r\n",
        "      else:\r\n",
        "        \r\n",
        "        #print(img_tensor.shape)\r\n",
        "        print(\"Pretrained initial state\")\r\n",
        "\r\n",
        "      \r\n",
        "      print('Low_res_img')\r\n",
        "      show_tensor_images(real, size=(real_dim, target_shape, target_shape) , switch=False)\r\n",
        "\r\n",
        "\r\n",
        "      print('high --> low')\r\n",
        "      show_tensor_images(hl1, size=(real_dim, target_shape, target_shape) , switch=False)\r\n",
        "      print('high --> low')\r\n",
        "      show_tensor_images(hl2, size=(real_dim, target_shape, target_shape) , switch= False)\r\n",
        "\r\n",
        "      print('High_res_img')\r\n",
        "      show_tensor_images(condition, size=(input_dim, target_shape, target_shape) , switch=False)\r\n",
        "\r\n",
        "      print('low --> high')\r\n",
        "      show_tensor_images(lh1, size=(real_dim, target_shape, target_shape) , switch=False)\r\n",
        "      print('low --> high')\r\n",
        "      show_tensor_images(lh2, size=(real_dim, target_shape, target_shape) , switch=False)\r\n",
        "\r\n",
        "      print('low , low --> High')\r\n",
        "      show_tensor_images(lh ,size=(real_dim, target_shape, target_shape) , switch=False )\r\n",
        "\r\n",
        "      mean_generator_loss = 0\r\n",
        "      mean_discriminator_loss = 0\r\n",
        "    cur_step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5IDMK5rmg1G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}